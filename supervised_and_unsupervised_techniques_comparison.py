# -*- coding: utf-8 -*-
"""supervised_and_unsupervised_techniques_comparison

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/supervised-and-unsupervised-techniques-comparison-e2ec531b-b107-4681-ab51-ef08fdeff75e.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240930/auto/storage/goog4_request%26X-Goog-Date%3D20240930T153315Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dc4c98f419798befcb42d66e20bd2c9bc502231fbbb89ed825f1c6934a44718935d3614369812576d24001b99f72f746a55acb498315ea0c4f33c5cdf36c4e66a0202bb4a0e90ab182b2fb2b28659e0263a07079bc03f733b50f2b66d833d83128f0e1c8345abcdc85f41b23170fccc894d5f7bdcc65b0ec75c00362b437f39f98846f56cae46108d98f0fdbda06ec5a9760f221fa7b3901df03952445c1157b72292db9de1152638c06a2fb12c762623d8494165dde2f53f55f62ab7fe67746cad28618ea7494dbe50c3a63387d280d6634779ceab9d2f97cb2c6c0f757fb898dbc4f9a2f29b3878d94bf544c6935bdc63b770ef5dfd877f55ac75ce751043ef
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'heart-2020-cleaned:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5794307%2F9517430%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240930%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240930T153314Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Daaec689a56fb1a36bc500a1a8e49ab268138b6340a073464e4966f86fe17f181ca09642a10367648a1deadb29df13182079c9aa7d05f68cf93e8cffb3cbcc29bf9ee519c396db9bee8cdb3c0f0aba49db53127bd9107d7956e8513d9ba0765b456b4cf562531fa20fecf61181a8c9a53803109fb827403a07f160549b9ff945b4f05dbb925afad707e55487cdd58d554c2bb3beff2cf5db986d9caa12287cb4262ba4550df328fe7b7e4d91f0d2cb0e3bf3d593f76213e16795380e05335450eb03a21aa992ed28e1e55c64831c3ba87e84780949bfbde772916ab02413a4905a37b0f28a78aabb4c65481d4bde3243636ebeb259cbbd40130079b53c0479cdd'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('/kaggle/input/heart-2020-cleaned/heart_2020_cleaned.csv')
data.head()
data.describe()
data.info()

"""# **Data Preprocessing**"""

# Encoding categorical variables
label_encoders = {}
categorical_columns = ['Smoking', 'AlcoholDrinking', 'Stroke', 'DiffWalking', 'Sex', 'AgeCategory', 'Race',
                       'Diabetic', 'PhysicalActivity', 'GenHealth', 'Asthma', 'KidneyDisease', 'SkinCancer']

for column in categorical_columns:
    label_encoders[column] = LabelEncoder()
    data[column] = label_encoders[column].fit_transform(data[column])

# Encoding the target variable 'HeartDisease'
data['HeartDisease'] = label_encoders['HeartDisease'] = LabelEncoder().fit_transform(data['HeartDisease'])

# Features and labels
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Dimensionality Reduction with PCA (reduce to 10 components)
pca = PCA(n_components=10)
X_pca = pca.fit_transform(X_train_scaled)

"""# **Supervised Learning Model**"""

# Supervised Learning Technique 1: Logistic Regression
log_reg_model = LogisticRegression(random_state=42)
log_reg_model.fit(X_train_scaled, y_train)
y_pred_log_reg = log_reg_model.predict(X_test_scaled)

# Supervised Learning Technique 2: Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Evaluation: Logistic Regression
log_reg_report = classification_report(y_test, y_pred_log_reg, output_dict=True)

# Evaluation: Random Forest
rf_report = classification_report(y_test, y_pred_rf, output_dict=True)

# Display comparison results
log_reg_acc = accuracy_score(y_test, y_pred_log_reg)
rf_acc = accuracy_score(y_test, y_pred_rf)

# Show reports and accuracy
supervised_comparison = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest'],
    'Accuracy': [log_reg_acc, rf_acc],
    'Precision': [log_reg_report['1']['precision'], rf_report['1']['precision']],
    'Recall': [log_reg_report['1']['recall'], rf_report['1']['recall']],
    'F1-Score': [log_reg_report['1']['f1-score'], rf_report['1']['f1-score']]
})

print(supervised_comparison)

"""# **Unsupervised Model**"""

# Unsupervised Learning Technique 1: MiniBatch K-Means Clustering (more efficient for large datasets)
mini_kmeans_model = MiniBatchKMeans(n_clusters=2, random_state=42, batch_size=1000)
kmeans_labels = mini_kmeans_model.fit_predict(X_pca)

# Unsupervised Learning Technique 2: Hierarchical Clustering with a sample (sample 10% of the data)
data_sample = X_pca[:len(X_pca)//10]  # Taking a 10% sample
hierarchical_model = AgglomerativeClustering(n_clusters=2)
hierarchical_labels = hierarchical_model.fit_predict(data_sample)

# Silhouette Scores
kmeans_silhouette = silhouette_score(X_pca, kmeans_labels)
hierarchical_silhouette = silhouette_score(data_sample, hierarchical_labels)

# Display results
unsupervised_comparison = pd.DataFrame({
    'Model': ['MiniBatch K-Means', 'Hierarchical Clustering (Sample)'],
    'Silhouette Score': [kmeans_silhouette, hierarchical_silhouette]
})

print(unsupervised_comparison)

"""# **Data Visualizations**"""

# Correlation Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Features')
plt.show()

# Countplot of Heart Disease Distribution
plt.figure(figsize=(8,6))
sns.countplot(x='HeartDisease', data=data, palette='Set2')
plt.title('Distribution of Heart Disease')
plt.show()

# Boxplot of BMI by HeartDisease
plt.figure(figsize=(10,6))
sns.boxplot(x='HeartDisease', y='BMI', data=data, palette='Set3')
plt.title('Boxplot of BMI by Heart Disease')
plt.show()

# Model Performance Visualization for Supervised Models
plt.figure(figsize=(10,6))
supervised_comparison.set_index('Model').plot(kind='bar', rot=0)
plt.title('Supervised Learning Model Comparison')
plt.ylabel('Score')
plt.legend(loc='lower right')
plt.show()

# Model Performance Visualization for unsupervised Models
plt.figure(figsize=(10,6))
supervised_comparison.set_index('Model').plot(kind='bar', rot=0)
plt.title('Unsupervised Learning Model Comparison')
plt.ylabel('Score')
plt.legend(loc='lower right')
plt.show()

# Model Performance Visualization for Unsupervised Models
plt.figure(figsize=(10,6))

# Plot the silhouette scores for the unsupervised models
unsupervised_comparison.set_index('Model').plot(kind='bar', rot=0, legend=False)

plt.title('Unsupervised Learning Model Comparison')
plt.ylabel('Silhouette Score')
plt.legend(loc='lower right')
plt.show()